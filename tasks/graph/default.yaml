task:
  name: graph
  output_dir_name: graph
  batch_size: 128
  target_metric: val_rocauc
  target_metric_mode: max
  max_epochs: 100  # adamcpr with the givend efault keeps learning over 100 epochs, some settings are done in 25
  max_steps: null
  model:  # GIN
    hidden_channels: 300
    num_layers: 3
    activation: gelu  # any from torch.nn.activation, try: leaky_relu
    dropout: 0.2
    graph_pooling: add  # {max, mean, add}
    jumping_knowledge: null  # {null, "last", "cat", "max", "lstm"} https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GIN.html#torch_geometric.nn.models.GIN
    mlp:
      hidden_channels: 128
      layers: 3
      activation: gelu  # any from torch.nn.activation, try: leaky_relu
      dropout: 0.2
      norm: "batch_norm"
engine:
  devices: 1
optimizer:
  name: adamcpr
  weight_decay: 0.001
  learning_rate: 1.77e-4
  kappa_init_param: 1
evaluation:
  plot:
    metric: test_rocauc
    test_metric_mode: max
    format: "2.1"
    limits: [60, 75]
