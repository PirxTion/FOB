engine: 
  accelerator: gpu           # Whether to train on cpu or gpu
  check_finite: true         # Check if target metric is finite during training.Aborts training if not.
  data_dir: ./data           # Where you want to store the training data
  deterministic: warn        # 'warn' tries to use deterministic algorithms if possible, also accepts true or false.
  detect_anomaly: false      # Lightning trainer argument with same name.
  devices: null              # This is set by each task by default, but can be overridden
  early_stopping: null       # The number of epochs to wait before stopping if no improvement is found. Set to null to disable.
  gradient_clip_alg: norm    # {value, norm} to disable gradient clipping: set 'gradient_clip_val' to null
  gradient_clip_val: null    # DEFAULT: don't clip gradients, expects value in [0, 1]
  log_extra: false           # Activate logging of gradients and more.
  optimize_memory: false     # Use nondeterministic, but memory-efficient algorithms for self-attention
  output_dir: ./experiments  # Where you want to store the results
  precision: bf16-mixed      # Floating precision of training, see https://lightning.ai/docs/pytorch/stable/common/precision_basic.html
  resume: true               # You can either pass the path to your checkpoint here or set to true, which loads the last checkpoint.
  run_scheduler: sequential  # How to schedule the runs of the experiment. Supported values:
                               # 'sequential': runs are performed sequentially 
                               # 'single:N' where N is the number of the run starting from 1.
                               # 'slurm_array': runs are scheduled using a SLURM array job.
                               # 'slurm_jobs': runs are scheduled using independent SLURM jobs
  sbatch_args:               # Additional arguments to pass to sbatch. Only used if run_scheduler is 'slurm_array'.
    # ntasks-per-node and gres are set to 'devices' by default
    # cpus-per-task is set to 'workers' by default
    nodes: 1
    mem-per-cpu: 2gb
    time: 00:30:00           # each task has their own default time (assumes A100 or similar gpu)
  sbatch_script_template: null  # Path to template for the sbatch script. Script can contain placeholder '__FOB_COMMAND__'. Otherwise it will be executed before the experiment. 'sbatch_args' will be added to the beginning of the script.
  slurm_log_dir: null        # Default: 'output_dir/slurm_logs' for run_scheduler 'slurm_array' and 'run_dir/slurm_logs' for run_scheduler 'slurm_jobs'
  seed: 42                   # The seed to use for the experiment
  seed_mode: fixed           # Currently only supports 'fixed'
  silent: false              # whether to hide progress bars. Recommended when writing outputs to a log file.
  test: true                 # Whether to test the model.
  train: true                # Whether to train the model.
  workers: 16                # The number of processes to use for dataloading
