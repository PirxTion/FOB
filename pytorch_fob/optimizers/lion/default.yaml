# Warmup + AdamW + CosineAnnealing
optimizer:
  name: lion
  learning_rate: 1.e-4
  one_minus_beta1: 0.1
  beta2: 0.999
  warmup_factor: 0.01   # factor of the total steps used for warmup
  warmup_steps: null    # number of steps used for warmup, overrides warmup_factor
  weight_decay: 0.0
  decoupled_weight_decay: false  # whether to use decoupled weight decay
  eta_min_factor: 0.01  # the minimum learning rate of the cosine annealing given as a factor of the initial learning rate
  lr_interval: step     # epoch is possible but step is preferred
  lr_scheduler: cosine  # can be either 'cosine' or 'poly'
  lr_power: 1.0         # only used if lr_scheduler == 'poly're
