optimizer:
  name: sgd_stepwise             # same as folder name
  output_dir_name: sgd_stepwise  # if you want a different name for the output folder, defaults to optimizer name if removed
  lr_interval: epoch             # can be either 'step' or 'epoch'
  # optimizer (from torch.optim import SGD)
  learning_rate: 1.e-2           # SGD parameter
  momentum: 0.9                  # SGD parameter
  weight_decay: 0.0              # SGD parameter
  # Scheduler (from torch.optim.lr_scheduler import StepLR)
  step_size: 10                  # interval
  gamma: 0.2                     # lr factor
  last_epoch: -1                 # for early stopping of the steps
