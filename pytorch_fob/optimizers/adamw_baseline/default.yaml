# Warmup + AdamW + CosineAnnealing
optimizer:
  name: adamw_baseline  # same as folder name
  learning_rate: 1.e-3  # AdamW parameter 
  beta1: 0.9            # AdamW parameter
  beta2: 0.999          # AdamW parameter
  weight_decay: 0.01    # AdamW parameter
  epsilon: 1.e-8        # AdamW parameter
  lr_scheduler:
    scheduler: cosine        # can be either 'cosine', 'poly' or 'wsd'
    eta_min_factor: 0.01     # the minimum learning rate of the lr scheduler given as a factor of the base learning rate
    lr_interval: step        # epoch is possible but step is preferred
    warmup_factor: 0.01      # factor of the total steps used for warmup
    warmup_steps: null       # number of steps used for warmup, overrides warmup_factor
    warmup_strategy: linear  # can be either 'linear' or 'cosine'

    # scheduler dependent args:
    lr_power: 1.0            # only used if lr_scheduler == 'poly'
    decay_strategy: cosine   # only used if lr_scheduler == 'wsd',can be either 'linear' or 'cosine'
    decay_factor: 0.1        # only used if lr_scheduler == 'wsd'
    decay_steps: null        # only used if lr_scheduler == 'wsd', overrides decay_factor
