# Warmup + AdamW + CosineAnnealing
optimizer:
  name: adamw_baseline  # same as folder name
  learning_rate: 1.e-3  # AdamW parameter 
  one_minus_beta1: 0.1  # 1 - beta1 AdamW parameter
  beta2: 0.999          # AdamW parameter
  warmup_factor: 0.01   # factor of the total steps used for warmup
  warmup_steps: null    # number of steps used for warmup, overrides warmup_factor
  weight_decay: 0.01    # AdamW parameter
  epsilon: 1.e-8        # AdamW parameter
  eta_min_factor: 0.01  # the minimum learning rate of the cosine annealing given as a factor of the initial learning rate
  lr_interval: step     # epoch is possible but step is preferred
  lr_scheduler: cosine  # can be either 'cosine' or 'poly'
  lr_power: 1.0         # only used if lr_scheduler == 'poly'
