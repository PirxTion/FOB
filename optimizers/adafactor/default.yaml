# Warmup + Adafactor
optimizer:
  name: adafactor       # same as folder name
  learning_rate: 1.e-3
  one_minus_beta1: 0.1  # 1 - beta1 parameter
  weight_decay: 0.01    # parameter
  eps1: 1.0e-30
  eps2: 1.0e-3
  clipping_threshold: 1.0
  decay_rate: -0.8
  lr_interval: step     # epoch is possible but step is preferred
