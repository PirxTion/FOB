# Warmup + AdamW + CosineAnnealing
optimizer:
  name: adamw_baseline
  output_dir_name: adamw_baseline
  learning_rate: 1.e-3  # AdamW parameter 
  one_minus_beta1: 0.1  # 1 - beta1 AdamW parameter
  beta2: 0.999          # AdamW parameter
  warmup_factor: 0.01   # factor of the total steps used for warmup
  weight_decay: 0.01    # AdamW parameter
  epsilon: 1.e-8        # AdamW parameter
  eta_min_factor: 0.01  # the minimum learning rate of the cosine annealing given as a factor of the initial learning rate
  lr_interval: step     # epoch is possible but step is preferred
